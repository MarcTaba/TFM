{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb230dc0",
   "metadata": {},
   "source": [
    "# Sample Siamese Network Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5a4fe",
   "metadata": {},
   "source": [
    "**The training was interrupted, the network reaches on average accuracies larger than 0.75!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640605c5",
   "metadata": {},
   "source": [
    "Additionally a Random Forest and a Logistic Regression were fitted, but accuracies were notably below than the ones obtained in the Siamese Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5f5f9",
   "metadata": {},
   "source": [
    "The code were trained with positve and negative samples, postive was a seen item followed by a purchase in the same session, the negative the contrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b624652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46935647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all the data:\n",
    "items = pd.read_csv(\"dressipi_recsys2022/item_features.csv\")\n",
    "candidate = pd.read_csv(\"dressipi_recsys2022/candidate_items.csv\")\n",
    "purchase =  pd.read_csv(\"dressipi_recsys2022/train_purchases.csv\")\n",
    "sessions =  pd.read_csv(\"dressipi_recsys2022/train_sessions.csv\")\n",
    "sessions_test = pd.read_csv(\"dressipi_recsys2022/test_final_sessions.csv\")\n",
    "leaderboard = pd.read_csv(\"dressipi_recsys2022/test_leaderboard_sessions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53be9c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23691"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.item_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73eab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the date column to a date format\n",
    "purchase['date'] = pd.to_datetime(purchase['date'])\n",
    "#list of unique items:\n",
    "items_unique = sessions.item_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414697c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_day = purchase['date'].max() #max date 31/05/2021 \n",
    "min_day = max_day -datetime.timedelta(31)\n",
    "cond = purchase['date'] >= min_day\n",
    "new_purchases = purchase[cond]\n",
    "\n",
    "#top 99 most sold items in the last month of the training data\n",
    "top_seller = new_purchases['item_id'].value_counts()[0:100].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64f2bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4743820it [00:15, 309445.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sessions_item_dict = {}\n",
    "item_sessions_dict = {}\n",
    "for i,x in tqdm(enumerate(zip(sessions['session_id'], sessions['item_id']))):\n",
    "    session, item = x\n",
    "    if session not in sessions_item_dict:\n",
    "        sessions_item_dict[session] = {}\n",
    "    \n",
    "    if item not in item_sessions_dict:\n",
    "        item_sessions_dict[item] = {}\n",
    "        \n",
    "    if item not in sessions_item_dict[session]:\n",
    "        sessions_item_dict[session][item] = 0\n",
    "    \n",
    "    if session not in item_sessions_dict[item]:\n",
    "        item_sessions_dict[item][session] = 0\n",
    "        \n",
    "    sessions_item_dict[session][item] += 1\n",
    "    item_sessions_dict[item][session] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f515f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_item_dict_test = {}\n",
    "item_sessions_dict_test = {}\n",
    "for i,x in enumerate(zip(sessions_test['session_id'], sessions_test['item_id'])):\n",
    "    session, item = x\n",
    "    if session not in sessions_item_dict_test:\n",
    "        sessions_item_dict_test[session] = {}\n",
    "    \n",
    "    if item not in item_sessions_dict_test:\n",
    "        item_sessions_dict_test[item] = {}\n",
    "        \n",
    "    if item not in sessions_item_dict_test[session]:\n",
    "        sessions_item_dict_test[session][item] = 0\n",
    "    \n",
    "    if session not in item_sessions_dict_test[item]:\n",
    "        item_sessions_dict_test[item][session] = 0\n",
    "        \n",
    "    sessions_item_dict_test[session][item] += 1\n",
    "    item_sessions_dict_test[item][session] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d40b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_item_dict_lad = {}\n",
    "item_sessions_dict_lad = {}\n",
    "for i,x in enumerate(zip(leaderboard['session_id'], leaderboard['item_id'])):\n",
    "    session, item = x\n",
    "    if session not in sessions_item_dict_lad:\n",
    "        sessions_item_dict_lad[session] = {}\n",
    "    \n",
    "    if item not in item_sessions_dict_lad:\n",
    "        item_sessions_dict_lad[item] = {}\n",
    "        \n",
    "    if item not in sessions_item_dict_lad[session]:\n",
    "        sessions_item_dict_lad[session][item] = 0\n",
    "    \n",
    "    if session not in item_sessions_dict_lad[item]:\n",
    "        item_sessions_dict_lad[item][session] = 0\n",
    "        \n",
    "    sessions_item_dict_lad[session][item] += 1\n",
    "    item_sessions_dict_lad[item][session] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58bc8211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23496/23496 [00:00<00:00, 37896.71it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_positive(max_num = 10):\n",
    "    '''\n",
    "    Generates Samples of Products seen in the same Session\n",
    "    with a maximum amount of max_num items per item\n",
    "    '''\n",
    "    positive = []\n",
    "    positive_dict = {}\n",
    "    for item in tqdm(item_sessions_dict):\n",
    "        cont = 0\n",
    "        for session in item_sessions_dict[item]:\n",
    "            for elem in sessions_item_dict[session]:\n",
    "                positive.append([item,elem])\n",
    "                cont += 1\n",
    "                if item not in positive_dict:\n",
    "                    positive_dict[item] = []\n",
    "                positive_dict[item].append(elem)\n",
    "                if(cont == max_num): break\n",
    "            if(cont == max_num): break\n",
    "                \n",
    "    return positive, positive_dict\n",
    "\n",
    "positive_sample,positive_dict = generate_positive() #generates data of positive samples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aff507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23496/23496 [00:01<00:00, 21151.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_negative(max_num = int(len(positive_sample)/len(items_unique))):\n",
    "    '''Generates Samples of Products not seen in the same Session\n",
    "    with a maximum amount of max_num items per item'''\n",
    "    negative = []\n",
    "    for item in tqdm(items_unique):\n",
    "        cont = 0\n",
    "        while(cont < max_num):\n",
    "            negative_sample = random.choice(items_unique)\n",
    "            if negative_sample not in positive_dict[item] and negative_sample != item:\n",
    "                negative.append([item,negative_sample])\n",
    "                cont += 1\n",
    "    return negative   \n",
    "\n",
    "negative_sample = generate_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2f4949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 471751/471751 [02:10<00:00, 3608.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_item_features():\n",
    "    items_features = {}\n",
    "    new_item = 2\n",
    "    features = []\n",
    "    for i in tqdm(range(len(items))):\n",
    "        if(items.iloc[i].item_id == new_item):\n",
    "            features.append(items.iloc[i].feature_category_id)\n",
    "            features.append(items.iloc[i].feature_value_id)\n",
    "        else:\n",
    "            items_features[new_item] = features\n",
    "            new_item = items.iloc[i].item_id\n",
    "            features = []\n",
    "            features.append(items.iloc[i].feature_category_id)\n",
    "            features.append(items.iloc[i].feature_value_id)\n",
    "    items_features[new_item] = features\n",
    "    return items_features\n",
    "item_features = get_item_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c54fa",
   "metadata": {},
   "source": [
    "The largest number of description is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46c43dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 471751/471751 [00:00<00:00, 873894.69it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxi = 0\n",
    "for i in tqdm(items.item_id):\n",
    "    if(len(item_features[i]) > maxi): maxi = len(item_features[i])\n",
    "maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90259f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484a29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    full_dataset = np.vstack((positive_sample,negative_sample))\n",
    "    full_dataset = pd.DataFrame(full_dataset)\n",
    "    full_dataset['label'] = 0\n",
    "    full_dataset['label'].iloc[0:len(positive_sample)-1] = 1\n",
    "    train = full_dataset.iloc[:,0:2]\n",
    "    test = full_dataset.iloc[:,2:3]    \n",
    "    X_train,X_val,y_train,y_val = train_test_split(train,test,test_size = 0.2,random_state = 42,shuffle = True)\n",
    "    return X_train,X_val,y_train,y_val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "462ee0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3460ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_dataset(df):\n",
    "    max_length = 132\n",
    "    feature_vector = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        x1 = df.iloc[i][0]\n",
    "        x2 = df.iloc[i][1]\n",
    "        x1_features = item_features[x1]\n",
    "        x2_features = item_features[x2]\n",
    "        combine_features = np.zeros(max_length)\n",
    "        combine_features[0:len(x1_features)] = x1_features\n",
    "        combine_features[int(max_length/2):int(max_length/2)+len(x2_features)] = x2_features\n",
    "        feature_vector.append(combine_features)\n",
    "    return feature_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b29908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 345598/345598 [01:11<00:00, 4853.95it/s]\n",
      "100%|██████████| 86400/86400 [00:17<00:00, 4952.46it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_feature = generate_feature_dataset(X_train)\n",
    "X_test_feature = generate_feature_dataset(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40ed6d",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "314ba4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marct\\anaconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\marct\\anaconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=0)\n",
    "model.fit(X_train_feature,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49f16ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.5480934496148705\n",
      "The validation accuracy is 0.5457175925925926\n"
     ]
    }
   ],
   "source": [
    "print(f\"The training accuracy is {model.score(X_train_feature, y_train)}\")\n",
    "print(f\"The validation accuracy is {model.score(X_test_feature, y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9132ab",
   "metadata": {},
   "source": [
    "Note that we do require a bit more of iterations to fully converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ed2b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = model.predict_proba(X_test_feature)\n",
    "good_matches = []\n",
    "for i in range(len(probabilities)):\n",
    "    if(probabilities[i,0]<= 0.40): good_matches.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82a2ff45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4198"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9eaf8",
   "metadata": {},
   "source": [
    "Alguna coseta es pot fer segur aquí veient això."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c6d79",
   "metadata": {},
   "source": [
    "### Random Forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e7befe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marct\\AppData\\Local\\Temp\\ipykernel_11204\\4279261279.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train_feature,y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train_feature,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d11009b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is 0.9998032396020811\n",
      "The validation accuracy is 0.7235532407407408\n"
     ]
    }
   ],
   "source": [
    "print(f\"The training accuracy is {model.score(X_train_feature, y_train)}\")\n",
    "print(f\"The validation accuracy is {model.score(X_test_feature, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f55a545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = model.predict_proba(X_test_feature)\n",
    "good_matches = []\n",
    "for i in range(len(probabilities)):\n",
    "    if(probabilities[i,0]<= 0.35):\n",
    "        good_matches.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d429e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = {}\n",
    "first = np.array(X_train[0])\n",
    "second = np.array(X_train[1])\n",
    "for i in good_matches:\n",
    "    if(first[i] not in matching):\n",
    "        matching[first[i]] = []\n",
    "    if(second[i] not in matching):\n",
    "        matching[second[i]] = []\n",
    "    matching[first[i]].append(second[i])\n",
    "    matching[second[i]].append(first[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e19587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution_rf():\n",
    "    \n",
    "    solution = [[]]\n",
    "    \n",
    "    for x in tqdm(leaderboard.session_id.unique()):\n",
    "        seen = 0\n",
    "        for k in sessions_item_dict_lad[x]:\n",
    "            if k in matching:\n",
    "                for j in matching[k]:\n",
    "                    solution.append([int(x),int(j),seen+1])\n",
    "                    seen += 1\n",
    "                    if(seen == 100): break\n",
    "                if(seen == 100): break\n",
    "            if(seen == 100): break\n",
    "        for j in range(100-seen):\n",
    "            solution.append([int(x),int(top_seller[j]),seen+1])\n",
    "            seen += 1\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7aa9f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:09<00:00, 5363.74it/s] \n"
     ]
    }
   ],
   "source": [
    "solution = generate_solution_rf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd3f0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.pop(0)\n",
    "solution = pd.DataFrame(solution, columns = ['session_id','item_id', 'rank']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8d7a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_csv = solution.to_csv\n",
    "solution.to_csv(r'solution_csv.csv',index = False)\n",
    "#first simple dummy solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bb5fc",
   "metadata": {},
   "source": [
    "# Siamese (ConvNet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91b92aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Let's Proceed now to generate the actual Siamesian Network############\n",
    "#https://keras.io/examples/vision/siamese_contrastive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b702757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f91435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Now let's define the Siamese Network:\n",
    "input_size = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebca36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input((input_size,1))\n",
    "x = tf.keras.layers.BatchNormalization()(input)\n",
    "x = layers.Conv1D(128, 2, strides = 2, activation=\"tanh\")(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "embedding_network = keras.Model(input, x)\n",
    "\n",
    "\n",
    "input_1 = layers.Input((input_size,1))\n",
    "input_2 = layers.Input((input_size,1))\n",
    "\n",
    "tower_1 = embedding_network(input_1)\n",
    "tower_2 = embedding_network(input_2)\n",
    "\n",
    "merge_layer = layers.Lambda(euclidean_distance)([tower_1, tower_2])\n",
    "\n",
    "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
    "\n",
    "output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "\n",
    "siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27413aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(margin=1):\n",
    "    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "  Arguments:\n",
    "      margin: Integer, defines the baseline for distance for which pairs\n",
    "              should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "  Returns:\n",
    "      'constrastive_loss' function with data ('margin') attached.\n",
    "  \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "      Arguments:\n",
    "          y_true: List of labels, each label is of type float32.\n",
    "          y_pred: List of predictions of same length as of y_true,\n",
    "                  each label is of type float32.\n",
    "\n",
    "      Returns:\n",
    "          A tensor containing constrastive loss as floating point value.\n",
    "      \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7fef73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 66, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 66, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 33, 16)       11252       ['input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1, 16)        0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 1, 16)       64          ['lambda[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1, 1)         17          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,333\n",
      "Trainable params: 11,299\n",
      "Non-trainable params: 34\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "margin  = 1\n",
    "siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73b52bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 256\n",
    "margin = 1  # Margin for constrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b9919f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature_np =  np.asarray(X_train_feature)\n",
    "X_train_feature1 = list(X_train_feature_np[:,0:66])\n",
    "X_train_feature2 = list(X_train_feature_np[:,66:])\n",
    "X_test_feature_np = np.asarray(X_test_feature)\n",
    "X_test_feature1 = list(X_test_feature_np[:,0:66])\n",
    "X_test_feature2 = list(X_test_feature_np[:,66:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4f413ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1350/1350 [==============================] - 35s 24ms/step - loss: 0.2254 - accuracy: 0.6097 - val_loss: 0.2544 - val_accuracy: 0.5133\n",
      "Epoch 2/10\n",
      "1350/1350 [==============================] - 31s 23ms/step - loss: 0.2231 - accuracy: 0.6175 - val_loss: 0.2352 - val_accuracy: 0.6001\n",
      "Epoch 3/10\n",
      "1350/1350 [==============================] - 31s 23ms/step - loss: 0.2199 - accuracy: 0.6277 - val_loss: 0.2203 - val_accuracy: 0.6264\n",
      "Epoch 4/10\n",
      "1350/1350 [==============================] - 42s 31ms/step - loss: 0.2184 - accuracy: 0.6320 - val_loss: 0.2338 - val_accuracy: 0.6197\n",
      "Epoch 5/10\n",
      "1350/1350 [==============================] - 40s 29ms/step - loss: 0.2171 - accuracy: 0.6357 - val_loss: 0.2178 - val_accuracy: 0.6347\n",
      "Epoch 6/10\n",
      "1350/1350 [==============================] - 41s 30ms/step - loss: 0.2163 - accuracy: 0.6366 - val_loss: 0.2164 - val_accuracy: 0.6341\n",
      "Epoch 7/10\n",
      "1350/1350 [==============================] - 41s 31ms/step - loss: 0.2170 - accuracy: 0.6346 - val_loss: 0.2201 - val_accuracy: 0.6251\n",
      "Epoch 8/10\n",
      "1350/1350 [==============================] - 41s 31ms/step - loss: 0.2158 - accuracy: 0.6379 - val_loss: 0.2171 - val_accuracy: 0.6407\n",
      "Epoch 9/10\n",
      "1350/1350 [==============================] - 39s 29ms/step - loss: 0.2153 - accuracy: 0.6402 - val_loss: 0.2191 - val_accuracy: 0.6228\n",
      "Epoch 10/10\n",
      "1350/1350 [==============================] - 41s 30ms/step - loss: 0.2149 - accuracy: 0.6407 - val_loss: 0.2206 - val_accuracy: 0.6377\n"
     ]
    }
   ],
   "source": [
    "history = siamese.fit(\n",
    "    [np.array(X_train_feature1), np.array(X_train_feature2)],\n",
    "    np.array(y_train.astype(\"float32\")),\n",
    "    validation_data=([np.array(X_test_feature1),np.array(X_test_feature2)], np.array(y_val.astype(\"float32\"))),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = siamese.fit(\n",
    "  #  [X_train_features[0:67], X_train_features[67:],\n",
    "  #  np.array(y_train.astype(\"float32\")),\n",
    "  #  validation_data=([np.array(X_val.astype(\"float32\"))[:,0], np.array(X_val.astype(\"float32\"))[:,1]], np.array(y_val.astype(\"float32\"))),\n",
    "  #  batch_size=batch_size,\n",
    "  #  epochs=epochs,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b838f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_metric(history, metric, title, has_valid=True):\n",
    "    \"\"\"Plots the given 'metric' from 'history'.\n",
    "\n",
    "    Arguments:\n",
    "        history: history attribute of History object returned from Model.fit.\n",
    "        metric: Metric to plot, a string value present as key in 'history'.\n",
    "        title: A string to be used as title of plot.\n",
    "        has_valid: Boolean, true if valid data was passed to Model.fit else false.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    plt.plot(history[metric])\n",
    "    if has_valid:\n",
    "        plt.plot(history[\"val_\" + metric])\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the accuracy\n",
    "plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9d300",
   "metadata": {},
   "source": [
    "# Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e97235",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_unique = sessions_test.item_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfabf729",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_item_dict = {}\n",
    "item_sessions_dict = {}\n",
    "for i,x in enumerate(zip(sessions_test['session_id'], sessions_test['item_id'])):\n",
    "    session, item = x\n",
    "    if session not in sessions_item_dict:\n",
    "        sessions_item_dict[session] = {}\n",
    "    \n",
    "    if item not in item_sessions_dict:\n",
    "        item_sessions_dict[item] = {}\n",
    "        \n",
    "    if item not in sessions_item_dict[session]:\n",
    "        sessions_item_dict[session][item] = 0\n",
    "    \n",
    "    if session not in item_sessions_dict[item]:\n",
    "        item_sessions_dict[item][session] = 0\n",
    "        \n",
    "    sessions_item_dict[session][item] += 1\n",
    "    item_sessions_dict[item][session] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094faf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positive(max_num = 25):\n",
    "    '''\n",
    "    Generates Samples of Products seen in the same Session\n",
    "    with a maximum amount of max_num items per item\n",
    "    '''\n",
    "    positive = []\n",
    "    positive_dict = {}\n",
    "    for item in tqdm(item_sessions_dict):\n",
    "        cont = 0\n",
    "        for session in item_sessions_dict[item]:\n",
    "            for elem in sessions_item_dict[session]:\n",
    "                positive.append([item,elem])\n",
    "                cont += 1\n",
    "                if item not in positive_dict:\n",
    "                    positive_dict[item] = []\n",
    "                positive_dict[item].append(elem)\n",
    "                if(cont == max_num): break\n",
    "            if(cont == max_num): break\n",
    "                \n",
    "    return positive, positive_dict\n",
    "\n",
    "positive_sample,positive_dict = generate_positive() #generates data of positive samples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a50bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative(max_num = 25):\n",
    "    '''Generates Samples of Products not seen in the same Session\n",
    "    with a maximum amount of max_num items per item'''\n",
    "    negative = []\n",
    "    for item in tqdm(items_unique):\n",
    "        for i in range(max_num):\n",
    "            negative_sample = random.choice(items_unique)\n",
    "            if negative_sample not in positive_dict[item] and negative_sample != item:\n",
    "                negative.append([item,negative_sample])\n",
    "    return negative   \n",
    "\n",
    "negative_sample = generate_negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    full_dataset = np.vstack((positive_sample,negative_sample))\n",
    "    full_dataset = pd.DataFrame(full_dataset)\n",
    "    full_dataset['label'] = 0\n",
    "    full_dataset['label'].iloc[0:len(positive_sample)-1] = 1\n",
    "    train = full_dataset.iloc[:,0:2]\n",
    "    test = full_dataset.iloc[:,2:3]    \n",
    "    X_train,X_val,y_train,y_val = train_test_split(train,test,test_size = 0.99999,random_state = 42,shuffle = True)\n",
    "    return X_train,X_val,y_train,y_val  \n",
    "X_train,X_test,y_train,y_test = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = siamese.evaluate([np.array(X_test.astype(\"float32\"))[:,0], np.array(X_test.astype(\"float32\"))[:,1]], np.array(y_test.astype(\"float32\")))\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = siamese.predict([np.array(X_test.astype(\"float32\"))[:,0], np.array(X_test.astype(\"float32\"))[:,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28ea7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
